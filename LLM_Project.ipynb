{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Q8LCXHOIU4Rv",
        "outputId": "a7c1e0a6-9ea2-47da-e7ea-508850b33d27"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/NUST Bank-Product-Knowledge.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-79d91ae1fcc9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Extract the basic question-answer pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mextracted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcel_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Print the extracted data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-79d91ae1fcc9>\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m(excel_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mall_qa_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mxls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msheet_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/NUST Bank-Product-Knowledge.xlsx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_data(excel_file):\n",
        "    all_qa_pairs = []\n",
        "    xls = pd.ExcelFile(excel_file)\n",
        "    sheet_names = xls.sheet_names\n",
        "\n",
        "    for i, sheet_name in enumerate(sheet_names):\n",
        "        if i < 2:\n",
        "            continue\n",
        "\n",
        "        df = pd.read_excel(xls, sheet_name, header=None)\n",
        "        qa_pairs = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            first_non_empty_index = -1\n",
        "            first_non_empty_value = None\n",
        "            for col_index, cell in enumerate(row):\n",
        "                if pd.notna(cell):\n",
        "                    first_non_empty_index = col_index\n",
        "                    first_non_empty_value = str(cell).strip()\n",
        "                    break\n",
        "\n",
        "            if first_non_empty_value and first_non_empty_value.endswith('?'):\n",
        "                question = first_non_empty_value\n",
        "                answer = \"\"\n",
        "                for next_index in range(index + 1, len(df)):\n",
        "                    next_row = df.iloc[next_index]\n",
        "                    next_first_non_empty_index = -1\n",
        "                    next_first_non_empty_value = None\n",
        "                    for col_index, cell in enumerate(next_row):\n",
        "                        if pd.notna(cell):\n",
        "                            next_first_non_empty_index = col_index\n",
        "                            next_first_non_empty_value = str(cell).strip()\n",
        "                            break\n",
        "\n",
        "                    if next_first_non_empty_value and not next_first_non_empty_value.endswith('?'):\n",
        "                        answer += next_first_non_empty_value + \" \"\n",
        "                    elif next_first_non_empty_value and next_first_non_empty_value.endswith('?'):\n",
        "                        break\n",
        "                    elif all(pd.isna(cell) for cell in next_row):\n",
        "                        break\n",
        "                if question and answer:\n",
        "                    qa_pairs.append({'question': question, 'answer': answer.strip()})\n",
        "\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "    return all_qa_pairs\n",
        "\n",
        "excel_file_path = '/content/NUST Bank-Product-Knowledge.xlsx'\n",
        "\n",
        "# Extract the basic question-answer pairs\n",
        "extracted_data = extract_data(excel_file_path)\n",
        "\n",
        "# Print the extracted data\n",
        "for item in extracted_data:\n",
        "    print(f\"Question: {item['question']}\")\n",
        "    print(f\"Answer: {item['answer']}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "# Save the extracted data to JSON\n",
        "import json\n",
        "with open('basic_qa_pairs.json', 'w') as f:\n",
        "    json.dump(extracted_data, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "Fr_5v4-MVIOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load merged QA data\n",
        "with open('basic_qa_pairs.json', 'r') as f:\n",
        "    qa_data = json.load(f)\n",
        "\n",
        "# Convert to prompt-response format\n",
        "train_data = [{\"prompt\": f\"Q: {item['question']}\\nA:\", \"response\": item['answer']} for item in qa_data]\n",
        "\n",
        "# Save as Hugging Face-style dataset\n",
        "with open('train_data.json', 'w') as f:\n",
        "    json.dump(train_data, f, indent=2)\n"
      ],
      "metadata": {
        "id": "jt9_8z_xinB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "KzOXjGi2i-3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load your dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"train_data.json\")[\"train\"]\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(batch):\n",
        "    full_texts = [p + \" \" + r for p, r in zip(batch[\"prompt\"], batch[\"response\"])]\n",
        "    return tokenizer(full_texts, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "DhRhj5y1ipBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Load in 8-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare for LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, config)\n"
      ],
      "metadata": {
        "id": "1b-Fqv0NirS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek-qa-finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "W9cCgBcjital"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"deepseek-qa-finetuned\")\n",
        "tokenizer.save_pretrained(\"deepseek-qa-finetuned\")"
      ],
      "metadata": {
        "id": "Ohq5khoTivlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model (local or from Hugging Face Hub)\n",
        "model_path = \"./deepseek-qa-finetuned\"  # or \"your-username/deepseek-qa-finetuned\" if uploaded\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\").eval()\n"
      ],
      "metadata": {
        "id": "A4M2uHRMixRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, max_tokens=256):\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Remove the question part and only return the answer\n",
        "    return generated_text.split(\"A:\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "v_QdJWqxizO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is NUST4Car?\"\n",
        "answer = generate_answer(question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "QhfRFkcai1O9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}